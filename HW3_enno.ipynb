{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598934aa",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as rq\n",
    "import bs4\n",
    "import lxml\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from dateutil import parser\n",
    "import pickle \n",
    "import functools\n",
    "import heapq\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e3e3e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5cdeac-5870-49a4-9f6e-c4e46fea2100",
   "metadata": {},
   "source": [
    "The methods in this section have been used to aquire all the data later used in this notebook.\n",
    "Since we uploaded all the data to GitHub and the methods take a long time to complete, it is not\n",
    "recommended to run them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bcb07a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1.1 Get the list of places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbe7bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_places(top_places_url=\"https://www.atlasobscura.com/places?sort=likes_count\"):\n",
    "    \"\"\"\n",
    "    This method fetches all the URLs from atlasobscura and writes them to a .txt file.\n",
    "    Disclaimer: It takes a long time!\n",
    "    \"\"\"\n",
    "    \n",
    "    home_url = \"https://www.atlasobscura.com\"\n",
    "    top_places = []\n",
    "    N = 400 # number of pages\n",
    "    \n",
    "    for _ in range(N): \n",
    "\n",
    "        if _ % 50 == 0: # print process\n",
    "            print(page_url)\n",
    "\n",
    "        time.sleep(0.01) # sleep to avoid too many requests\n",
    "        result = rq.get(page_url)\n",
    "        soup = bs4.BeautifulSoup(result.text)\n",
    "\n",
    "        # acess all 18 cards of the current page\n",
    "        cards = soup.find_all(\"div\", {\"class\": \"col-md-4 col-sm-6 col-xs-12\"})\n",
    "        # len(cards) = 18\n",
    "\n",
    "        # only store the href for each card\n",
    "        page_urls = [card.find_all(\"a\", {\"class\": \"content-card content-card-place\"})[0].get('href') \n",
    "                     for card in cards]\n",
    "\n",
    "        top_places.append(page_urls)\n",
    "\n",
    "        # acess url for the next page from the \"next page\" button\n",
    "        next_page = soup.find_all(\"a\", {\"rel\": \"next\"})[0].get('href')\n",
    "        page_url = home_url + next_page\n",
    "    \n",
    "    # write all urls to one .txt file\n",
    "    with open(\"places.txt\", 'w+') as f:\n",
    "        for page in top_places:\n",
    "            for place in page:\n",
    "                f.write(place + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90e376a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of places fetched: 7200\n"
     ]
    }
   ],
   "source": [
    "with open(\"places.txt\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print('Number of places fetched:', len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21dc8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1.2 Crawl places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e55661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for writing .html files for the set of urls for one page\n",
    "def crawl_places(path_to_place_urls,\n",
    "                 parent_dir=\"htmls\"):\n",
    "    \"\"\"\n",
    "    This method reads the URLs from the places.txt and creates folders according to the pages\n",
    "    containing the .html files for each place.\n",
    "    \"\"\"\n",
    "    \n",
    "    # read and split urls in 400 sets of 18\n",
    "    with open(path_to_place_urls, 'r+') as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "        top_places = np.array_split(lines, 400)\n",
    "    \n",
    "    # iterate through pages\n",
    "    for page_ix, urls_per_page in enumerate(top_places, start=1):\n",
    "        # create folder per page\n",
    "        directory = f'page_{page_ix}'\n",
    "        path = os.path.join(parent_dir, directory)\n",
    "        os.mkdir(path)\n",
    "        \n",
    "        # iterate through places per page\n",
    "        for place_ref in urls_per_page:\n",
    "            place_ref = place_ref[:-1]\n",
    "\n",
    "            time.sleep(5) # sleep to avoid too many requests error\n",
    "            page = rq.get(place_ref)\n",
    "            place_name = place_ref.split('/')[-1].replace('-', '_')\n",
    "            \n",
    "            # write to .html file\n",
    "            with open(f'{path}/{place_name}.html', 'wb+') as f:\n",
    "                f.write(page.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5205e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1.3  Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdf35a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_place_attributes(soup, tsv_dir):\n",
    "    \"\"\"\n",
    "    This method creates extracts all the attributes from the html code and writes it to a \n",
    "    tsv file.\n",
    "    \"\"\"\n",
    "    \n",
    "    placeName = str(soup.find_all('h1',{'class':'DDPage__header-title'})[0].contents[0])\n",
    "    \n",
    "    try:\n",
    "        placeTags = str(soup.find_all('div',{'class':'DDPage__header-place-location'})[0].contents[0].text).split(sep=',')\n",
    "    except:\n",
    "        placeTags = ''\n",
    "    \n",
    "    numPeopleVisited = int(soup.find_all('div', {'class':'title-md item-action-count'})[0].contents[0])\n",
    "    \n",
    "    numPeopleWant = int(soup.find_all('div', {'class':'title-md item-action-count'})[-1].contents[0])\n",
    "    \n",
    "    placeDesc = soup.find_all('div', {'class':'DDP__body-copy'})[0].text.strip()\n",
    "    \n",
    "    placeShortDesc = soup.find_all('h3', {'class':'DDPage__header-dek'})[0].text\n",
    "    \n",
    "    placeAddress = soup.find_all('address', {'class':'DDPageSiderail__address'})[0].contents[3].text.partition('\\n')[0]\n",
    "    \n",
    "    list_Alt_Long = soup.find_all('address', {'class':'DDPageSiderail__address'})[0].contents[3].text\n",
    "    \n",
    "    placeAlt = int(float(list_Alt_Long.split('\\n', 1)[1].split()[0].replace(',', ' ')))\n",
    "    \n",
    "    placeLong = int(float(list_Alt_Long.split('\\n', 1)[1].split()[1].replace(',', ' ')))\n",
    "\n",
    "    placeEditors = soup.find_all('div', {'class':'js-editor-list hidden'})\n",
    "    if len(placeEditors) == 0:\n",
    "        placeEditors = list()\n",
    "    else:\n",
    "        placeEditors = soup.find_all('div', {'class':'js-editor-list hidden'})[0].text.split()\n",
    "    \n",
    "    for elem in placeEditors:\n",
    "        if len(elem) == 1:\n",
    "            placeEditors.remove(elem)\n",
    "\n",
    "    placePubDate = soup.find_all('div', {'class':'DDPContributor__name'})\n",
    "    if len(placePubDate) > 0:\n",
    "        placePubDate = placePubDate[0].text.replace(',', ' ')\n",
    "        placePubDate = parser.parse(placePubDate).date() # %y-%m-%d\n",
    "    else:\n",
    "        placePubDate = ''\n",
    "        \n",
    "    card_grids = soup.find_all('div', \n",
    "                               {'class': \"card-grid CardRecircSection__card-grid js-inject-gtm-data-in-child-links\"\n",
    "    })\n",
    "    \n",
    "    if len(card_grids) >= 2:\n",
    "        second_card = card_grids[1]\n",
    "        placeRelatedPlaces = [button.get(\"data-place-title\") \n",
    "                              for button in second_card.find_all('div', \n",
    "                                                                 {'class': 'Card__action-btns vue-js-been-there-everywhere-place'})]\n",
    "    else:\n",
    "        placeRelatedPlaces = ''\n",
    "\n",
    "    if len(card_grids) >= 3:\n",
    "        third_card = card_grids[2]\n",
    "        placeRelatedLists = [related_list.text.strip()\n",
    "                             for related_list in third_card.find_all('h3', \n",
    "                                                                     {'class': 'Card__heading --content-card-v2-title js-title-content'})]\n",
    "    else:\n",
    "        placeRelatedLists = ''\n",
    "\n",
    "    placeURL = soup.find_all('link', {'rel': \"canonical\"})[0].get('href')\n",
    "\n",
    "    df = pd.DataFrame({'placeName' : placeName, \n",
    "                      'placeTags' : [placeTags],\n",
    "                      'numPeopleVisited': numPeopleVisited,\n",
    "                      'numPeopleWant': numPeopleWant,\n",
    "                      'placeDesc': placeDesc,\n",
    "                      'placeShortDesc': placeShortDesc,\n",
    "                      'placeAddress': placeAddress, \n",
    "                      'placeAlt': placeAlt,\n",
    "                      'placeLong': placeLong,\n",
    "                      'placeEditors': [placeEditors],\n",
    "                      'placePubDate': placePubDate,\n",
    "                      'placeRelatedPlaces': [placeRelatedPlaces],\n",
    "                      'placeRelatedLists': [placeRelatedLists],\n",
    "                      'placeURL': placeURL})\n",
    "    \n",
    "    file_name = '_'.join(placeName.lower().split()).replace('/', '_')\n",
    "    file_name=''.join(e for e in file_name if (e=='_' or e.isalnum())) # to remove special characters that are not '_' in file_name\n",
    "\n",
    "    data = df.to_csv(f'{tsv_dir}{file_name}.tsv', sep=\"\\t\")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ed086c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_tsv(path_to_tsv_dir='tsvs',\n",
    "                path_to_html_dir='htmls'):\n",
    "    \n",
    "    \"\"\"\n",
    "    This method first lists all 400 page folders and then converts the html files per page\n",
    "    to a tsv file for each place.\n",
    "    \"\"\"\n",
    "    \n",
    "    pages = sorted(os.listdir(path_to_html_dir), \n",
    "                   key=lambda dir_name: int(dir_name.split('_')[-1])) # sort by page number\n",
    "\n",
    "    for current_page in pages:\n",
    "        path_to_current_page = os.path.join(path_to_html_dir, current_page)\n",
    "        places_per_page = os.listdir(path_to_current_page) # list places per page\n",
    "\n",
    "        for current_place in places_per_page:\n",
    "            path_to_current_place = os.path.join(path_to_current_page, current_place)\n",
    "\n",
    "            with open(path_to_current_place, 'rb') as f:\n",
    "                soup = bs4.BeautifulSoup(f.read(), 'lxml') \n",
    "                get_place_attributes(soup, path_to_tsv_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea0176-3cab-42ac-b5f3-2838f2faac0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Search Engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f76802-7f72-4da8-9b79-3bad1ee2fecf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now, we want to create two different Search Engines that, given as input a query, return the places that match the query.\n",
    "\n",
    "First, you must pre-process all the information collected for each place by:\n",
    "\n",
    "   1 Removing stopwords\n",
    "   \n",
    "   2 Removing punctuation\n",
    "   \n",
    "   3 Stemming\n",
    "   \n",
    "   4 Anything else you think it's needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4f74124-112b-4d12-bcb7-bc461ced8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "snowstem = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b749dbc-b3bb-40c5-8500-abfc62082dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsv_to_dataframe(path_to_tsv_files='tsvs/tsvs', \n",
    "                      load_file=False,\n",
    "                      save_to_file=False):\n",
    "    \"\"\"\n",
    "    This function reads and preprocesses the information contained in the tsv files.\n",
    "    Preprocessing includes removing punctuation, removing stop words and stemming.\n",
    "    \n",
    "    Using the load_file=True flag, will load the already computed .tsv file from the disk,\n",
    "    insted of computing it.\n",
    "    \"\"\"\n",
    "    \n",
    "    if load_file:\n",
    "        df = pd.read_csv('merged_places.tsv', delimiter = ',')\n",
    "    else:\n",
    "        dfs = []\n",
    "        for file in os.listdir(path_to_tsv_files):  # loop over the files in the folder\n",
    "            data = pd.read_csv(os.path.join(path_to_tsv_files, file), delimiter='\\t')  \n",
    "\n",
    "            filtered_sentence = ' '.join(tokenizer.tokenize(data.placeDesc[0])) # removing punctuation\n",
    "            word_tokens = word_tokenize(filtered_sentence) # tokenize words\n",
    "            filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] # removing stop_words\n",
    "            filtered_sentence = [snowstem.stem(word) for word in filtered_sentence] # stemming  \n",
    "\n",
    "            data['cleanDesc'] = filtered_sentence\n",
    "            dfs.append(data)\n",
    "\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    if save_to_file:\n",
    "        df.to_csv('merged_places.tsv')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a276c67b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "places_df = tsv_to_dataframe(load_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f88b1bc7-88d4-4394-8067-63645f78ff40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeTags</th>\n",
       "      <th>numPeopleVisited</th>\n",
       "      <th>numPeopleWant</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeShortDesc</th>\n",
       "      <th>placeAddress</th>\n",
       "      <th>placeAlt</th>\n",
       "      <th>placeLong</th>\n",
       "      <th>placeEditors</th>\n",
       "      <th>placePubDate</th>\n",
       "      <th>placeRelatedPlaces</th>\n",
       "      <th>placeRelatedLists</th>\n",
       "      <th>placeURL</th>\n",
       "      <th>cleanDesc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Tarkiln Bayou Preserve State Park</td>\n",
       "      <td>['Pensacola', ' Florida']</td>\n",
       "      <td>126</td>\n",
       "      <td>514</td>\n",
       "      <td>This rugged, swampy Florida state park is home...</td>\n",
       "      <td>One of the best places in Florida to spot rare...</td>\n",
       "      <td>2401 Bauer RdPensacola, FloridaUnited States</td>\n",
       "      <td>30</td>\n",
       "      <td>-87</td>\n",
       "      <td>[]</td>\n",
       "      <td>2018-05-02</td>\n",
       "      <td>['Fragas do Eume', 'Lange Duinen', 'Darlington...</td>\n",
       "      <td>['15 Florida Places to Be Nurtured By Nature',...</td>\n",
       "      <td>https://www.atlasobscura.com/places/tarkiln-ba...</td>\n",
       "      <td>[rug, swampi, florida, state, park, home, mani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Hemingway's Last Penny</td>\n",
       "      <td>['Key West', ' Florida']</td>\n",
       "      <td>772</td>\n",
       "      <td>440</td>\n",
       "      <td>In the gardens of Hemingway’s beautiful home i...</td>\n",
       "      <td>An idyllic swimming pool tells the tale of Hem...</td>\n",
       "      <td>907 Whitehead StreetKey West, Florida, 33040Un...</td>\n",
       "      <td>24</td>\n",
       "      <td>-81</td>\n",
       "      <td>[]</td>\n",
       "      <td>2014-02-17</td>\n",
       "      <td>[\"Scoresby's Polar Bear\", '‘Fantasy Swan’', 'L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.atlasobscura.com/places/hemingway-...</td>\n",
       "      <td>[garden, hemingway, beauti, home, key, west, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Ponyhenge</td>\n",
       "      <td>['Lincoln', ' Massachusetts']</td>\n",
       "      <td>438</td>\n",
       "      <td>1258</td>\n",
       "      <td>On a small slice of wide-open pasture in the t...</td>\n",
       "      <td>No one really knows how these old hobby horses...</td>\n",
       "      <td>47 Old Sudbury RdLincoln, Massachusetts, 01773...</td>\n",
       "      <td>42</td>\n",
       "      <td>-71</td>\n",
       "      <td>['TheDiscoveryService', 'bailey1618', 'beautif...</td>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>['Clarkes Collectibles &amp; Lunchbox Museum', 'Ta...</td>\n",
       "      <td>['Outdoor Toy Gardens and Graveyards', 'There ...</td>\n",
       "      <td>https://www.atlasobscura.com/places/ponyhenge</td>\n",
       "      <td>[small, slice, wide, open, pastur, town, linco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>The Athenian Agora</td>\n",
       "      <td>['Athens', ' Greece']</td>\n",
       "      <td>1570</td>\n",
       "      <td>767</td>\n",
       "      <td>Lying right beneath the northern slope of the ...</td>\n",
       "      <td>This ancient Greek place of assembly and marke...</td>\n",
       "      <td>46 AdrianouAthens, 105 55Greece</td>\n",
       "      <td>37</td>\n",
       "      <td>23</td>\n",
       "      <td>['Joel', 'Cusumano', 'Aika', 'antonette', 'Mor...</td>\n",
       "      <td>2010-09-16</td>\n",
       "      <td>['Tullaherin Folk Museum and Round Tower', 'Ca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.atlasobscura.com/places/the-atheni...</td>\n",
       "      <td>[lie, right, beneath, northern, slope, acropol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Calla Lily Valley</td>\n",
       "      <td>['Carmel-By-The-Sea', ' California']</td>\n",
       "      <td>107</td>\n",
       "      <td>544</td>\n",
       "      <td>If you’re traveling along Highway 1, this is t...</td>\n",
       "      <td>This little valley along Highway 1 is engulfed...</td>\n",
       "      <td>Garrapata TrailCarmel-By-The-Sea, California, ...</td>\n",
       "      <td>36</td>\n",
       "      <td>-121</td>\n",
       "      <td>[]</td>\n",
       "      <td>2019-11-16</td>\n",
       "      <td>['Mazama Queen Mine', 'Jam Up Cave', 'Paper Ai...</td>\n",
       "      <td>['18 Places Where You Just Have to Stop and Sm...</td>\n",
       "      <td>https://www.atlasobscura.com/places/calla-lily...</td>\n",
       "      <td>[travel, along, highway, 1, ideal, place, scen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          placeName  \\\n",
       "0           0  Tarkiln Bayou Preserve State Park   \n",
       "1           0             Hemingway's Last Penny   \n",
       "2           0                          Ponyhenge   \n",
       "3           0                 The Athenian Agora   \n",
       "4           0                  Calla Lily Valley   \n",
       "\n",
       "                              placeTags  numPeopleVisited  numPeopleWant  \\\n",
       "0             ['Pensacola', ' Florida']               126            514   \n",
       "1              ['Key West', ' Florida']               772            440   \n",
       "2         ['Lincoln', ' Massachusetts']               438           1258   \n",
       "3                 ['Athens', ' Greece']              1570            767   \n",
       "4  ['Carmel-By-The-Sea', ' California']               107            544   \n",
       "\n",
       "                                           placeDesc  \\\n",
       "0  This rugged, swampy Florida state park is home...   \n",
       "1  In the gardens of Hemingway’s beautiful home i...   \n",
       "2  On a small slice of wide-open pasture in the t...   \n",
       "3  Lying right beneath the northern slope of the ...   \n",
       "4  If you’re traveling along Highway 1, this is t...   \n",
       "\n",
       "                                      placeShortDesc  \\\n",
       "0  One of the best places in Florida to spot rare...   \n",
       "1  An idyllic swimming pool tells the tale of Hem...   \n",
       "2  No one really knows how these old hobby horses...   \n",
       "3  This ancient Greek place of assembly and marke...   \n",
       "4  This little valley along Highway 1 is engulfed...   \n",
       "\n",
       "                                        placeAddress  placeAlt  placeLong  \\\n",
       "0       2401 Bauer RdPensacola, FloridaUnited States        30        -87   \n",
       "1  907 Whitehead StreetKey West, Florida, 33040Un...        24        -81   \n",
       "2  47 Old Sudbury RdLincoln, Massachusetts, 01773...        42        -71   \n",
       "3                    46 AdrianouAthens, 105 55Greece        37         23   \n",
       "4  Garrapata TrailCarmel-By-The-Sea, California, ...        36       -121   \n",
       "\n",
       "                                        placeEditors placePubDate  \\\n",
       "0                                                 []   2018-05-02   \n",
       "1                                                 []   2014-02-17   \n",
       "2  ['TheDiscoveryService', 'bailey1618', 'beautif...   2016-06-30   \n",
       "3  ['Joel', 'Cusumano', 'Aika', 'antonette', 'Mor...   2010-09-16   \n",
       "4                                                 []   2019-11-16   \n",
       "\n",
       "                                  placeRelatedPlaces  \\\n",
       "0  ['Fragas do Eume', 'Lange Duinen', 'Darlington...   \n",
       "1  [\"Scoresby's Polar Bear\", '‘Fantasy Swan’', 'L...   \n",
       "2  ['Clarkes Collectibles & Lunchbox Museum', 'Ta...   \n",
       "3  ['Tullaherin Folk Museum and Round Tower', 'Ca...   \n",
       "4  ['Mazama Queen Mine', 'Jam Up Cave', 'Paper Ai...   \n",
       "\n",
       "                                   placeRelatedLists  \\\n",
       "0  ['15 Florida Places to Be Nurtured By Nature',...   \n",
       "1                                                NaN   \n",
       "2  ['Outdoor Toy Gardens and Graveyards', 'There ...   \n",
       "3                                                NaN   \n",
       "4  ['18 Places Where You Just Have to Stop and Sm...   \n",
       "\n",
       "                                            placeURL  \\\n",
       "0  https://www.atlasobscura.com/places/tarkiln-ba...   \n",
       "1  https://www.atlasobscura.com/places/hemingway-...   \n",
       "2      https://www.atlasobscura.com/places/ponyhenge   \n",
       "3  https://www.atlasobscura.com/places/the-atheni...   \n",
       "4  https://www.atlasobscura.com/places/calla-lily...   \n",
       "\n",
       "                                           cleanDesc  \n",
       "0  [rug, swampi, florida, state, park, home, mani...  \n",
       "1  [garden, hemingway, beauti, home, key, west, l...  \n",
       "2  [small, slice, wide, open, pastur, town, linco...  \n",
       "3  [lie, right, beneath, northern, slope, acropol...  \n",
       "4  [travel, along, highway, 1, ideal, place, scen...  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "places_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950fd1f6",
   "metadata": {},
   "source": [
    "## 2.1. Conjunctive query\n",
    "\n",
    "For the first version of the search engine, we narrow our interest to the description of each place. It means that you will evaluate queries only concerning the place's description.\n",
    "\n",
    "Note: You should use the longer description placeDesc column and not the short description placeShortDesc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ffa16471-c913-46a1-8a4f-edafc213f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(txt):\n",
    "    \"\"\"\n",
    "    This method cleans a given string like it is done with the placeDesc.\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_sentence = ' '.join(tokenizer.tokenize(txt)) # removing punctuation\n",
    "    word_tokens = word_tokenize(filtered_sentence) # tokenize string\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] # removing stop_words\n",
    "    filtered_sentence = [snowstem.stem(word) for word in filtered_sentence] # stemming \n",
    "    \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eec1b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(df=places_df, \n",
    "                      load_file=False,\n",
    "                      save_to_file=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Reads all preprocessed placeDesc and returns a dictionary with all words and their respective counts.\n",
    "    \n",
    "    Again, load_file=True saves computation time and reads the file from the disk.\n",
    "    \"\"\"\n",
    "    \n",
    "    if load_file:\n",
    "        with open('vocabulary.pkl', 'rb') as f:\n",
    "            vocabulary = pickle.load(f)\n",
    "    else:\n",
    "        vocabulary = Counter(functools.reduce(lambda x, y: x + y, df.cleanDesc))\n",
    "    \n",
    "    if save_to_file:\n",
    "        with open('vocabulary.pkl', 'wb') as f:\n",
    "            pickle.dump(vocabulary, f)\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2124b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = create_vocabulary(load_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0967928e-0dad-4d87-a7c3-1531203aea4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To get better understanding here is a slice of the vocabulary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rug': 70, 'swampi': 11, 'florida': 276, 'state': 2086, 'park': 3286}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('To get better understanding here is a slice of the vocabulary:')\n",
    "\n",
    "dict(list(vocabulary.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25a1006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_dict(vocabulary=vocabulary, \n",
    "                      load_file=False,\n",
    "                      save_to_file=False):\n",
    "    \"\"\"\n",
    "    Returns a dic that assigns each word of the vocabulary a unique integer ID.\n",
    "    \"\"\"\n",
    "    \n",
    "    if load_file:\n",
    "        with open('word_dict.pkl', 'rb') as f:\n",
    "            word_dict = pickle.load(f)\n",
    "    else:\n",
    "        word_dict = {}\n",
    "        item_id = 1\n",
    "\n",
    "        for word in vocabulary.keys():\n",
    "            word_dict[word] = item_id\n",
    "            item_id += 1\n",
    "    \n",
    "    if save_to_file:\n",
    "        with open('word_dict.pkl', 'wb') as f:\n",
    "            pickle.dump(word_dict, f)\n",
    "    \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04c2a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = create_word_dict(load_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20bf41",
   "metadata": {},
   "source": [
    "### 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2ef4dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(vocabulary=vocabulary, \n",
    "                          word_dict=word_dict, \n",
    "                          df=places_df, \n",
    "                          load_file=False,\n",
    "                          save_to_file=False):\n",
    "    \n",
    "    if load_file:\n",
    "        with open('inverted_idx.pkl', 'rb') as f:\n",
    "            inverted_idx = pickle.load(f)\n",
    "    else:\n",
    "        inverted_idx = {}\n",
    "        for word, item_id in word_dict.items():\n",
    "            inverted_idx[item_id] = list(df[df.cleanDesc.apply(lambda row: word in row)].index)\n",
    "\n",
    "    if save_to_file:\n",
    "        with open('inverted_idx.pkl', 'wb') as f:\n",
    "            pickle.dump(inverted_idx, f)\n",
    "    \n",
    "    return inverted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "898ce582",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx = create_inverted_index(load_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32add1d",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f4ebc8a3-db01-4353-a9df-4946eed01e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_query(query_str, \n",
    "                 inverted_idx=inverted_idx, \n",
    "                 word_dict=word_dict,\n",
    "                 df=places_df):\n",
    "    \"\"\"\n",
    "    Performs a simple query returning a df of places that contain all words of the query_str.\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = ['placeName', 'placeDesc', 'placeURL']\n",
    "    \n",
    "    query_str=cleaning(query_str) # cleaning the query\n",
    "    results = [set(inverted_idx[item]) for item in [word_dict[word] for word in query_str]]\n",
    "    \n",
    "    if len(results)>0:\n",
    "        \n",
    "        query_result = set.intersection(*map(set, results))\n",
    "        query_ix = df.loc[query_result][cols].index\n",
    "        \n",
    "        return places_df.loc[query_ix][cols]\n",
    "    else:\n",
    "        return print('No match')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b178ac6f-0a0c-44a3-bcce-b448c8f02868",
   "metadata": {},
   "source": [
    "Now we perform a simple query on the string: \n",
    "\n",
    "``american museum``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0a4d4629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253 places did contain the query: american museum \n",
      "Have a look at them!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4097</th>\n",
       "      <td>Museums at Old City Cemetery</td>\n",
       "      <td>Planning to travel back to 1899 to attend a fu...</td>\n",
       "      <td>https://www.atlasobscura.com/places/museums-at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3074</th>\n",
       "      <td>Evel Knievel Museum</td>\n",
       "      <td>The Evel Knievel Museum takes you through the ...</td>\n",
       "      <td>https://www.atlasobscura.com/places/evel-kniev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Athenian Agora</td>\n",
       "      <td>Lying right beneath the northern slope of the ...</td>\n",
       "      <td>https://www.atlasobscura.com/places/the-atheni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6150</th>\n",
       "      <td>Harvard Museum of Natural History</td>\n",
       "      <td>Collecting three different institutions into o...</td>\n",
       "      <td>https://www.atlasobscura.com/places/harvard-mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>99s Museum of Women Pilots</td>\n",
       "      <td>Women pilots had been barred from participatin...</td>\n",
       "      <td>https://www.atlasobscura.com/places/museum-of-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              placeName  \\\n",
       "4097       Museums at Old City Cemetery   \n",
       "3074                Evel Knievel Museum   \n",
       "3                    The Athenian Agora   \n",
       "6150  Harvard Museum of Natural History   \n",
       "1545         99s Museum of Women Pilots   \n",
       "\n",
       "                                              placeDesc  \\\n",
       "4097  Planning to travel back to 1899 to attend a fu...   \n",
       "3074  The Evel Knievel Museum takes you through the ...   \n",
       "3     Lying right beneath the northern slope of the ...   \n",
       "6150  Collecting three different institutions into o...   \n",
       "1545  Women pilots had been barred from participatin...   \n",
       "\n",
       "                                               placeURL  \n",
       "4097  https://www.atlasobscura.com/places/museums-at...  \n",
       "3074  https://www.atlasobscura.com/places/evel-kniev...  \n",
       "3     https://www.atlasobscura.com/places/the-atheni...  \n",
       "6150  https://www.atlasobscura.com/places/harvard-mu...  \n",
       "1545  https://www.atlasobscura.com/places/museum-of-...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text = 'american museum'\n",
    "simple_query_df = simple_query(query_text)\n",
    "\n",
    "print(len(simple_query_df), 'places did contain the query:', query_text, '\\nHave a look at them!')\n",
    "simple_query_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ca9eae",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e8951e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(df=places_df,\n",
    "              save_to_file=False,\n",
    "              load_file=False):\n",
    "    \"\"\"\n",
    "    Returns a df for that contains the tfIdf for all words and documents.\n",
    "    It is actually faster to calcualte it than loading it.\n",
    "    \"\"\"\n",
    "    \n",
    "    if load_file:\n",
    "        tfidf_df = pd.read_csv('tfidf.zip')\n",
    "        \n",
    "    else:\n",
    "\n",
    "        tfidf = TfidfVectorizer(input='content', lowercase=False, tokenizer=lambda text: text)\n",
    "        result = tfidf.fit_transform(df.cleanDesc)\n",
    "        dense_result = result.todense()\n",
    "        tfidf_df = pd.DataFrame(dense_result, \n",
    "                                index=df.index, \n",
    "                                columns=tfidf.get_feature_names_out())\n",
    "        \n",
    "    if save_to_file:\n",
    "        tfidf_df.to_csv('tfidf.csv')\n",
    "    \n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "44b70c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = get_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f7ec4868-1823-48e9-bf02-41188e66c774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000s</th>\n",
       "      <th>001</th>\n",
       "      <th>007</th>\n",
       "      <th>00am</th>\n",
       "      <th>00o</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>...</th>\n",
       "      <th>ма</th>\n",
       "      <th>родину</th>\n",
       "      <th>сло</th>\n",
       "      <th>⅜</th>\n",
       "      <th>⅝</th>\n",
       "      <th>万里长城</th>\n",
       "      <th>奥武島</th>\n",
       "      <th>猫神社</th>\n",
       "      <th>畳石</th>\n",
       "      <th>高徳院</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39781 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   00       000  000s  001  007  00am  00o  00pm   01  ...   ма  родину  \\\n",
       "0  0.0  0.0  0.000000   0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...  0.0     0.0   \n",
       "1  0.0  0.0  0.036829   0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...  0.0     0.0   \n",
       "2  0.0  0.0  0.000000   0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...  0.0     0.0   \n",
       "3  0.0  0.0  0.000000   0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...  0.0     0.0   \n",
       "4  0.0  0.0  0.000000   0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...  0.0     0.0   \n",
       "\n",
       "   сло    ⅜    ⅝  万里长城  奥武島  猫神社   畳石  高徳院  \n",
       "0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 39781 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679e6f8e",
   "metadata": {},
   "source": [
    "-TF of \"American\": nº times \"American\" appears in the query(1)/length of query(2).\n",
    "Therefore its TF would be 0.5\n",
    "\n",
    "-IDF of \"American\": log (nº of documents (which are 7200) / nº of documents in which the word \"American\" appears)\n",
    "\n",
    "-TFIDF of \"American\": = TF *IDF\n",
    "\n",
    "Then, we can do the same for the word \"Museum\" and obtain a vector that contains these two scores. FInally we compare its cosine similarity with every document that contains all words within the query (by transforming all of them into vectors of lenght 2 (TFIDF of \"American, TFIDF of \"museum) with the same procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "24180342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_tfidf(query, d_tfidf=tfidf):\n",
    "    \"\"\"\n",
    "    This method calcualtes the tfIdf for the query string.\n",
    "    The logic of this function follows the markdown box above, which is from a comment in slack.\n",
    "    \"\"\"\n",
    "    q = cleaning(query)\n",
    "    len_q = len(q)\n",
    "    len_d = len(d_tfidf)\n",
    "    tf = [Counter(q)[word]/len_q for word in q]\n",
    "    \n",
    "    idf = np.log([len_d/len(simple_query(word)) for word in q])\n",
    "    \n",
    "    q_tfidf = tf * idf\n",
    "    \n",
    "    return q_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6328d91-72e7-4586-b0b7-1a8ad3cd1941",
   "metadata": {},
   "source": [
    "$CosineSimilarity=\\frac{AB}{\\Vert A \\Vert \\Vert B \\Vert}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5cf30e07-7e65-4db6-8a26-f1d94c0b4c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(Q, D):\n",
    "    return np.dot(Q, D.T) / (np.linalg.norm(Q)*np.linalg.norm(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe1629-00dc-47df-82b9-f81e91415b6b",
   "metadata": {},
   "source": [
    "Now we perform a conjunctiv query and rank the results on the string: \n",
    "\n",
    "``colosseum rome``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ac3947d9-6d87-411f-a010-6ecf05b75c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query tfidf:\n",
      " [3.46012417 2.08740531]\n",
      "The tfIdf of the query in the body of documents:\n",
      "       colosseum      rome\n",
      "6654   0.045928  0.060938\n",
      "3580   0.077946  0.155128\n",
      "1085   0.085322  0.056603\n",
      "2604   0.109112  0.217156\n"
     ]
    }
   ],
   "source": [
    "query_str = 'colosseum rome'\n",
    "\n",
    "simple_query_df = simple_query(query_str) # get index from the simple query dataframe\n",
    "d_tfidf = tfidf_df.loc[simple_query_df.index][cleaning(query_str)] # get document tfIdf\n",
    "q_tfidf = get_query_tfidf(query_str) # get query tfidf\n",
    "print('The query tfidf:\\n', q_tfidf)\n",
    "print('The tfIdf of the query in the body of documents:\\n', d_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "76a7d6dd-6f6a-474a-bbac-e43b04216f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query string resulted in 4 matches:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6654</th>\n",
       "      <td>Shrine of Saints Magnus and Bonosa</td>\n",
       "      <td>How did the 1800-year-old bones of two Christi...</td>\n",
       "      <td>https://www.atlasobscura.com/places/the-shrine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3580</th>\n",
       "      <td>Rome's Gladiator School</td>\n",
       "      <td>Tucked away on a side-street of the Appian Way...</td>\n",
       "      <td>https://www.atlasobscura.com/places/romes-glad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>Cloaca Maxima</td>\n",
       "      <td>When the Romans built the Cloaca Maxima in the...</td>\n",
       "      <td>https://www.atlasobscura.com/places/cloaca-maxima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2604</th>\n",
       "      <td>Servian Wall at McDonald's</td>\n",
       "      <td>McDonald’s may be one of the last places you’d...</td>\n",
       "      <td>https://www.atlasobscura.com/places/servian-wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               placeName  \\\n",
       "6654  Shrine of Saints Magnus and Bonosa   \n",
       "3580            Rome's Gladiator School    \n",
       "1085                      Cloaca Maxima    \n",
       "2604          Servian Wall at McDonald's   \n",
       "\n",
       "                                              placeDesc  \\\n",
       "6654  How did the 1800-year-old bones of two Christi...   \n",
       "3580  Tucked away on a side-street of the Appian Way...   \n",
       "1085  When the Romans built the Cloaca Maxima in the...   \n",
       "2604  McDonald’s may be one of the last places you’d...   \n",
       "\n",
       "                                               placeURL  \n",
       "6654  https://www.atlasobscura.com/places/the-shrine...  \n",
       "3580  https://www.atlasobscura.com/places/romes-glad...  \n",
       "1085  https://www.atlasobscura.com/places/cloaca-maxima  \n",
       "2604  https://www.atlasobscura.com/places/servian-wa...  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The query string resulted in 4 matches:')\n",
    "simple_query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "04d30117",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = cosine_similarity(q_tfidf, d_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "540c3920-6aff-4ece-9267-4f02931f8639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After inserting the cosine similarity and sorting the simple query hits according to it, our df looks like this:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeURL</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2604</th>\n",
       "      <td>Servian Wall at McDonald's</td>\n",
       "      <td>McDonald’s may be one of the last places you’d...</td>\n",
       "      <td>https://www.atlasobscura.com/places/servian-wa...</td>\n",
       "      <td>0.632966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3580</th>\n",
       "      <td>Rome's Gladiator School</td>\n",
       "      <td>Tucked away on a side-street of the Appian Way...</td>\n",
       "      <td>https://www.atlasobscura.com/places/romes-glad...</td>\n",
       "      <td>0.452167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>Cloaca Maxima</td>\n",
       "      <td>When the Romans built the Cloaca Maxima in the...</td>\n",
       "      <td>https://www.atlasobscura.com/places/cloaca-maxima</td>\n",
       "      <td>0.314929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6654</th>\n",
       "      <td>Shrine of Saints Magnus and Bonosa</td>\n",
       "      <td>How did the 1800-year-old bones of two Christi...</td>\n",
       "      <td>https://www.atlasobscura.com/places/the-shrine...</td>\n",
       "      <td>0.217979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               placeName  \\\n",
       "2604          Servian Wall at McDonald's   \n",
       "3580            Rome's Gladiator School    \n",
       "1085                      Cloaca Maxima    \n",
       "6654  Shrine of Saints Magnus and Bonosa   \n",
       "\n",
       "                                              placeDesc  \\\n",
       "2604  McDonald’s may be one of the last places you’d...   \n",
       "3580  Tucked away on a side-street of the Appian Way...   \n",
       "1085  When the Romans built the Cloaca Maxima in the...   \n",
       "6654  How did the 1800-year-old bones of two Christi...   \n",
       "\n",
       "                                               placeURL  similarity  \n",
       "2604  https://www.atlasobscura.com/places/servian-wa...    0.632966  \n",
       "3580  https://www.atlasobscura.com/places/romes-glad...    0.452167  \n",
       "1085  https://www.atlasobscura.com/places/cloaca-maxima    0.314929  \n",
       "6654  https://www.atlasobscura.com/places/the-shrine...    0.217979  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_query_df['similarity'] = cosine_sim\n",
    "\n",
    "print('After inserting the cosine similarity and sorting the simple query hits according to it, our df looks like this:')\n",
    "simple_query_df.sort_values('similarity', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0278ad",
   "metadata": {},
   "source": [
    "## 3. Define a new score!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368a3619-51b0-4e85-95e4-cf9a8b56dd65",
   "metadata": {},
   "source": [
    "Our new score does is the average of the cosine similarity and a probability that is:\n",
    "$P(Visit)=\\frac{\\text{numPeopleVisited for a specific place}}{\\Sigma \\text{ numPeopleVisited of all simple query hits}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "83a267a7-71dc-4438-bb12-a74ca541841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare query\n",
    "query_str = 'colosseum rome'\n",
    "\n",
    "simple_query_df = simple_query(query_str)\n",
    "query_ix = simple_query_df.index\n",
    "d_tfidf = tfidf_df.loc[simple_query_df.index][cleaning(query_str)]\n",
    "q_tfidf = get_query_tfidf(query_str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "1c6099f8-6d02-4c92-9abc-c36543a9692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering only the matching places\n",
    "# we evaluate the probability that a person is looking for a specific place\n",
    "# adding this results to the cosine similarity to have a better search rank\n",
    "num_peop = []\n",
    "\n",
    "for num in places_df.loc[query_ix]['numPeopleVisited']:\n",
    "    num_peop.append(num)\n",
    "    \n",
    "total_people = sum(num_peop)\n",
    "probability_people = np.array(num_peop)/total_people \n",
    "\n",
    "q_tfidf = get_query_tfidf(query_str)\n",
    "cosine_sim = cosine_similarity(q_tfidf, tfidf_result)\n",
    "\n",
    "simple_query_df['cosine similarity'] = cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f79e6e2f-5b1c-499d-9d71-ef30757da5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scores = np.array(cosine_sim + probability_people) / 2 # we normalize dividing by 2 because each of the addend is in [0,1]\n",
    "simple_query_df['new similarity'] = new_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9e3ea601-5d00-486a-8e6d-48144aa5ef7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeURL</th>\n",
       "      <th>cosine similarity</th>\n",
       "      <th>new similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2604</th>\n",
       "      <td>Servian Wall at McDonald's</td>\n",
       "      <td>McDonald’s may be one of the last places you’d...</td>\n",
       "      <td>https://www.atlasobscura.com/places/servian-wa...</td>\n",
       "      <td>0.632966</td>\n",
       "      <td>0.471291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>Cloaca Maxima</td>\n",
       "      <td>When the Romans built the Cloaca Maxima in the...</td>\n",
       "      <td>https://www.atlasobscura.com/places/cloaca-maxima</td>\n",
       "      <td>0.314929</td>\n",
       "      <td>0.350734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3580</th>\n",
       "      <td>Rome's Gladiator School</td>\n",
       "      <td>Tucked away on a side-street of the Appian Way...</td>\n",
       "      <td>https://www.atlasobscura.com/places/romes-glad...</td>\n",
       "      <td>0.452167</td>\n",
       "      <td>0.282814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6654</th>\n",
       "      <td>Shrine of Saints Magnus and Bonosa</td>\n",
       "      <td>How did the 1800-year-old bones of two Christi...</td>\n",
       "      <td>https://www.atlasobscura.com/places/the-shrine...</td>\n",
       "      <td>0.217979</td>\n",
       "      <td>0.204182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               placeName  \\\n",
       "2604          Servian Wall at McDonald's   \n",
       "1085                      Cloaca Maxima    \n",
       "3580            Rome's Gladiator School    \n",
       "6654  Shrine of Saints Magnus and Bonosa   \n",
       "\n",
       "                                              placeDesc  \\\n",
       "2604  McDonald’s may be one of the last places you’d...   \n",
       "1085  When the Romans built the Cloaca Maxima in the...   \n",
       "3580  Tucked away on a side-street of the Appian Way...   \n",
       "6654  How did the 1800-year-old bones of two Christi...   \n",
       "\n",
       "                                               placeURL  cosine similarity  \\\n",
       "2604  https://www.atlasobscura.com/places/servian-wa...           0.632966   \n",
       "1085  https://www.atlasobscura.com/places/cloaca-maxima           0.314929   \n",
       "3580  https://www.atlasobscura.com/places/romes-glad...           0.452167   \n",
       "6654  https://www.atlasobscura.com/places/the-shrine...           0.217979   \n",
       "\n",
       "      new similarity  \n",
       "2604        0.471291  \n",
       "1085        0.350734  \n",
       "3580        0.282814  \n",
       "6654        0.204182  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_simple_query_df = simple_query_df.sort_values('new similarity', ascending=False)\n",
    "sorted_simple_query_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474d4e2c-d43d-494d-924c-0198f89b09cb",
   "metadata": {},
   "source": [
    "The see only the top $K$ results we simply call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "23de2e00-e4ae-4cd4-baa9-acaff285fb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeURL</th>\n",
       "      <th>cosine similarity</th>\n",
       "      <th>new similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2604</th>\n",
       "      <td>Servian Wall at McDonald's</td>\n",
       "      <td>McDonald’s may be one of the last places you’d...</td>\n",
       "      <td>https://www.atlasobscura.com/places/servian-wa...</td>\n",
       "      <td>0.632966</td>\n",
       "      <td>0.471291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>Cloaca Maxima</td>\n",
       "      <td>When the Romans built the Cloaca Maxima in the...</td>\n",
       "      <td>https://www.atlasobscura.com/places/cloaca-maxima</td>\n",
       "      <td>0.314929</td>\n",
       "      <td>0.350734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3580</th>\n",
       "      <td>Rome's Gladiator School</td>\n",
       "      <td>Tucked away on a side-street of the Appian Way...</td>\n",
       "      <td>https://www.atlasobscura.com/places/romes-glad...</td>\n",
       "      <td>0.452167</td>\n",
       "      <td>0.282814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       placeName  \\\n",
       "2604  Servian Wall at McDonald's   \n",
       "1085              Cloaca Maxima    \n",
       "3580    Rome's Gladiator School    \n",
       "\n",
       "                                              placeDesc  \\\n",
       "2604  McDonald’s may be one of the last places you’d...   \n",
       "1085  When the Romans built the Cloaca Maxima in the...   \n",
       "3580  Tucked away on a side-street of the Appian Way...   \n",
       "\n",
       "                                               placeURL  cosine similarity  \\\n",
       "2604  https://www.atlasobscura.com/places/servian-wa...           0.632966   \n",
       "1085  https://www.atlasobscura.com/places/cloaca-maxima           0.314929   \n",
       "3580  https://www.atlasobscura.com/places/romes-glad...           0.452167   \n",
       "\n",
       "      new similarity  \n",
       "2604        0.471291  \n",
       "1085        0.350734  \n",
       "3580        0.282814  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 3\n",
    "sorted_simple_query_df.head(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db1d49-0591-4230-bb5b-a74101581cae",
   "metadata": {},
   "source": [
    "#### This scores is better than the cosine similarity it because starts with it but considers also the probability given by the people visited.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8583f47-a0e7-47b4-8627-0ea0a98750ae",
   "metadata": {},
   "source": [
    "## 4. Visualizing the most relevant places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "722508c1-cbfd-4e8e-8ebf-aa29f6733080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e72bfb6-3acc-46fd-8d6e-a2e531237f79",
   "metadata": {},
   "source": [
    "For the sake of more hits than 4, we will fall back to the ``american museum`` query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "5aae62c1-0564-41a2-9ee5-ba8e49b8dab8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,) (253,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8139/2565521714.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mnew_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_sim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprobability_people\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;31m# we normalize dividing by 2 because each of the addend is in [0,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mvisual_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new similarity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,) (253,) "
     ]
    }
   ],
   "source": [
    "query_str = 'american museum'\n",
    "\n",
    "query_ix = simple_query(query_str).index\n",
    "visual_df = places_df.loc[query_ix]\n",
    "\n",
    "num_peop = []\n",
    "\n",
    "for num in visual_df['numPeopleVisited']:\n",
    "    num_peop.append(num)\n",
    "    \n",
    "total_people = sum(num_peop)\n",
    "\n",
    "probability_people = np.array(num_peop)/total_people \n",
    "\n",
    "q_tfidf = get_query_tfidf(query_str)\n",
    "cosine_sim = cosine_similarity(q_tfidf, tfidf_result)\n",
    "\n",
    "new_scores = np.array(cosine_sim + probability_people) / 2 # we normalize dividing by 2 because each of the addend is in [0,1]\n",
    "visual_df['new similarity'] = new_scores\n",
    "\n",
    "sorted_visual_df = visual_df.sort_values('new similarity', ascending=False)\n",
    "\n",
    "K = 10\n",
    "\n",
    "fig = px.scatter_mapbox(visual_df.head(K), \n",
    "                        lat=visual_df.head(K)['placeAlt'],\n",
    "                        lon=visual_df.head(K)['placeLong'], \n",
    "                        hover_name=visual_df.head(K)['placeAddress'],\n",
    "                        hover_data=[\"numPeopleVisited\"],\n",
    "                        color_discrete_sequence=[\"fuchsia\"], zoom=2, height=300)\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe988623",
   "metadata": {},
   "source": [
    "## 7. Theoretical question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a064b8f",
   "metadata": {},
   "source": [
    "An imaginary university is interested in accepting some of the applicants for positions to study the Master of Data Science there. Unfortunately, only a few spots are available, so the university requires students to take some exams. Students are then admitted based on how well they perform on these exams. For students to determine whether they have been successfully accepted to the university, the university wants to create a ranking list that includes every student's first name, last name, and total average on its course webpage. Students should be ranked in the list based on their average points in descending order. For example, if two students have the same average punctuation, they should be sorted in ascending order using their first and last names. University will give you the students' information in 'ApplicantsInfo.txt', and you should provide them with the ranking list in another .txt file and name it as 'RankingList.txt' . Kindly help this university in preparing this ranking list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69f8a5a",
   "metadata": {},
   "source": [
    "\n",
    "   1 Try solving the problem mentioned above using three different sorting algorithms (do not use any MapReduce algorithm). (Note: Built-in Python functions (like .mean, .sort, etc.) are not allowed to be used. You must implement the algorithms from scratch).\n",
    "   \n",
    "   2 What is the time complexity of each algorithm you have used?\n",
    "   \n",
    "   3 Evaluate the time taken for each of your implementations to answer the query stored in the ApplicantsInfo.txt file and visualize them.\n",
    "   \n",
    "   4 What is the most optimal algorithm, in your opinion, and why?\n",
    "   \n",
    "   5 Implement a sorting algorithm using MapReduce and compare it against the three algorithms previously implemented using the ApplicantsInfo.txt file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8275e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10000 #number of students  \n",
    "m=50 #number of exams\n",
    "f=open('ApplicantsInfo.txt','r')\n",
    "i=0\n",
    "Applic={} #we want to create a dict of students\n",
    "for line in f:\n",
    "    line=line.split()\n",
    "    if i==0: #skipping the first line because is the shape of the file\n",
    "        i=1\n",
    "    else:\n",
    "        if n!=0:\n",
    "            student=line[0]+' '+line[1]\n",
    "            Applic[student]=[int(x) for x in line[2:m+2]]\n",
    "            n-=1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "0f47dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing an average function\n",
    "def Average(x):\n",
    "    aver=0\n",
    "    for i in x:\n",
    "        aver+=i\n",
    "    \n",
    "    return round(aver/len(x),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04955316",
   "metadata": {},
   "source": [
    "The running time of Average is $\\Theta(N)$ where N is the length of the input x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "494948c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of lists with the averages for each student\n",
    "Rank=[]\n",
    "for name in [stud for stud in Applic.keys()]: \n",
    "    Rank.append([name,Average(Applic[name])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b3c8e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting algorithms for list of list\n",
    "#if h=1 it sorts by scores, if h=0 it sorts by names\n",
    "def InsertionSort(Rank,h): \n",
    "    \n",
    "    for i in range(1,len(Rank)):\n",
    "            temp= Rank[i]\n",
    "            j=i-1\n",
    "            while j>=0 and temp[h]>Rank[j][h]:\n",
    "                     Rank[j+1] = Rank[j]\n",
    "                     j-=1  \n",
    "            Rank[j+1] = temp\n",
    "    return Rank\n",
    "############################################\n",
    "def SelectionSort(Rank, h):\n",
    "    \n",
    "    for i in range(len(Rank)):\n",
    "        M = i\n",
    "        for j in range(i + 1,len(Rank)):\n",
    "            if Rank[j][h] > Rank[M][h]:\n",
    "                M = j\n",
    "        (Rank[i], Rank[M]) = (Rank[M], Rank[i])\n",
    "        \n",
    "    return Rank\n",
    "#############################################\n",
    "def BubbleSort(Rank,h):\n",
    "    change = 0\n",
    "    for i in range(len(Rank)-1):\n",
    "        for j in range(0, len(Rank)-i-1):\n",
    "            if Rank[j][h] < Rank[j+1][h]:\n",
    "                change = 1\n",
    "                (Rank[j], Rank[j+1]) = (Rank[j+1], Rank[j])\n",
    "        if change==0:\n",
    "            return Rank\n",
    "    return Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193bb0c4",
   "metadata": {},
   "source": [
    "All of those algorithms have running time of $\\Theta(N^2)$ where N is the length of the array in input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "9c335bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting the Applicants\n",
    "import timeit \n",
    "unsorted=Rank.copy() #we want to keep the unsorted list to compare this non MapReduced algorithm with the MapReduced one\n",
    "Times=[]\n",
    "for Alg in [1,2,3]: # 1: InsertionSort, 2: SelectionSort, 3:BubbleSort\n",
    "    Rank=unsorted.copy()\n",
    "    start=timeit.default_timer() #this is to evaluate the time required to run the algorithm\n",
    "    #sorting by exams averages\n",
    "    if Alg==1:\n",
    "        Rank=InsertionSort(Rank,1)\n",
    "    elif Alg==2:\n",
    "        Rank=SelectionSort(Rank,1)\n",
    "    elif Alg==3:\n",
    "        Rank=BubbleSort(Rank,1)\n",
    "        \n",
    "    #sorting duplicates by names\n",
    "    i=0 \n",
    "    while i <= (len(Rank)-1):\n",
    "        j=1\n",
    "        while (i+j)<(len(Rank)-1) and Rank[i][1]==Rank[i+j][1]: #to know the length of consecutive elements that have same score\n",
    "            j+=1\n",
    "        if j!=1: \n",
    "            #temp variable to store the correct order (we used [::-1] to sort in ascending order)\n",
    "            if Alg==1:\n",
    "                temp=InsertionSort([Rank[i+k] for k in range(j)],0)[::-1] \n",
    "            elif Alg==2:\n",
    "                temp=SelectionSort([Rank[i+k] for k in range(j)],0)[::-1]\n",
    "            if Alg==3:\n",
    "                temp=BubbleSort([Rank[i+k] for k in range(j)],0)[::-1]\n",
    "            for k in range(j):\n",
    "                Rank[i+k]=temp[k] #changing the order in the Rank\n",
    "            i+=j #just to skip the elements already changed\n",
    "        else:\n",
    "            i+=1\n",
    "\n",
    "    stop=timeit.default_timer()\n",
    "    Times.append(stop-start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625987a6",
   "metadata": {},
   "source": [
    "The running time to sort the duplicate is, in the worst case (i.e. all the scores are the same), equal to the running time of the sort method we use to sort by names so it is $\\Theta(N^2)$.\n",
    "\n",
    "So we have that to sort the Rank vector, in the worst case, we need a running time of $\\Theta(2N^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "8ac48acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAARFUlEQVR4nO3deZAmdX3H8fcHFpTDeO0WohIG0WARD4RRQY0xohWvgpQShVKURLOJVqJoygRjGU3KWGgZKyaKZIPGixAUMUFFFEuQ0oqY4ZBj8cAbRRkwongh+M0fTy8O4w7bczxPz8zv/aqamu5++un+/ujZ/tDXr1NVSJLatdPQBUiShmUQSFLjDAJJapxBIEmNMwgkqXEGgSQ1bmxBkOQdSa5LcsWcafdIcm6SL3e/7z6u9UuS+sm4niNI8ljgJuDdVfWgbtobgO9X1YlJTgDuXlV/vaNlbdy4saampsZSpyStVxdddNH1VbVpR/NtGFcBVXVBkql5k48EHtcNvws4H9hhEExNTTEzM7OS5UnSupfkG33mm/Q1gr2q6tpu+LvAXgvNmGRzkpkkM7Ozs5OpTpIaNNjF4hqdk1rwvFRVbamq6aqa3rRph0c2kqQlmnQQfC/J3gDd7+smvH5J0jyTDoKzgOd1w88D/nvC65ckzTPO20dPA/4HOCDJNUmeD5wIPDHJl4EndOOSpAGN866hYxb46PBxrVOStHg+WSxJjTMIJKlxBoEkNW5s1wgktWHqhI8MXcK69fUTnzqR9XhEIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkho3SBAkeWmSK5NckeS0JHceog5J0gBBkOQ+wIuB6ap6ELAzcPSk65AkjQx1amgDsFuSDcDuwHcGqkOSmjfxIKiqbwNvBL4JXAvcWFUfnz9fks1JZpLMzM7OTrpMSWrGEKeG7g4cCewH3BvYI8lz5s9XVVuqarqqpjdt2jTpMiWpGUOcGnoC8LWqmq2qXwBnAo8aoA5JEsMEwTeBQ5PsniTA4cBVA9QhSWKYawQXAmcAFwOXdzVsmXQdkqSRDUOstKpeDbx6iHVLkm7PJ4slqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNW6QIEhytyRnJPlCkquSHDZEHZIk2DDQet8MnFNVRyXZFdh9oDokqXkTD4IkdwUeCxwHUFU3AzdPug5J0sgQp4b2A2aBf09ySZJTkuwxQB2SJIYJgg3AwcDbquphwI+BE+bPlGRzkpkkM7Ozs5OuUZKaMUQQXANcU1UXduNnMAqG26mqLVU1XVXTmzZtmmiBktSSXkGQZI8kO3XDv5XkiCS7LGWFVfVd4FtJDugmHQ5sXcqyJEnL1/eI4ALgzknuA3wcOBZ45zLW+xfAqUkuAw4CXreMZUmSlqHvXUOpqp8keT5wUlW9IcmlS11pVV0KTC/1+5KkldP3iCDdQ1/PBj7STdt5PCVJkiapbxAcD7wC+GBVXZnkfsB5Y6tKkjQxvU4NVdWngE/NGf8q8OJxFSVJmpw7DIIkHwJqoc+r6ogVr0iSNFE7OiJ4Y/f76cC9gPd248cA3xtXUZKkybnDIOhOCZHkH6tq7l0+H0oyM9bKJEkT0fdi8R7dBWIAkuwH2D+QJK0DfZ8jeClwfpKvAgH2Bf50bFVJkiam711D5yR5APDAbtIXqurn4ytLkjQpi3kfwSHAVPedhyahqt49lqokSRPTKwiSvAfYH7gUuLWbXIBBIElrXN8jgmngwKpa8JkCSdLa1PeuoSsYPUcgSVpn+h4RbAS2JvkccNtFYp8slqS1r28QvGacRUiShtO707kkewEP7yZ9rqquG19ZkqRJ6fuqymcCnwP+EHgmcGGSo8ZZmCRpMvqeGnol8PBtRwFJNgGfYPTieUnSGtb3rqGd5p0KumER35UkrWJ9jwjOSfIx4LRu/FnAR8dTkiRpkvpeLH55kqcDj+kmbamqD46vLEnSpPTtYmI/4OyqOrMb3y3JVFV9fZzFSZLGr+95/vcDv5wzfms3TZK0xvUNgg1VdfO2kW541/GUJEmapL5BMJvktu4kkhwJXD+ekiRJk9T3rqE/A05N8lZG3U9fAzx3bFVJkiam711DXwEOTbJnN37TWKuSJE1M37uG9gJeB9y7qp6c5EDgsKp6+1irU5OmTvjI0CWsW18/8alDl6BVqO81gncCHwPu3Y1/CTh+DPVIkiasbxBsrKr30d1CWlW38KtXVkqS1rC+QfDjJPdkdKGYJIcCN46tKknSxPS9a+hlwFnA/kk+A2wC7IZaktaBvkcE+wNPBh7F6FrBl+kfIpKkVaxvELyqqn4I3B34PeAk4G1jq0qSNDF9g2DbheGnAv9WVR9hmV1MJNk5ySVJPryc5UiSlqdvEHw7yb8yeg/B2UnutIjvLuQlwFXLXIYkaZn67syfyejawO9X1Q+AewAvX+pKk9yX0dHFKUtdhiRpZfTtYuInwJlzxq8Frl3Gev8J+CvgLstYhiRpBUz8vcNJngZcV1UX7WC+zUlmkszMzs5OqDpJas8Qt4A+GjgiyVOAOwO/keS9VfWcuTNV1RZgC8D09HQtdWX2WzM+9lsjrQ8TPyKoqldU1X2rago4Gvjk/BCQJE3OxINAkrS6DPp0cFWdD5w/ZA2S1DqPCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMZNPAiS7JPkvCRbk1yZ5CWTrkGS9CsbBljnLcBfVtXFSe4CXJTk3KraOkAtktS8iR8RVNW1VXVxN/wj4CrgPpOuQ5I0Mug1giRTwMOAC7fz2eYkM0lmZmdnJ16bJLVisCBIsifwAeD4qvrh/M+raktVTVfV9KZNmyZfoCQ1YpAgSLILoxA4tarOHKIGSdLIEHcNBXg7cFVVvWnS65ck3d4QRwSPBo4FHp/k0u7nKQPUIUligNtHq+rTQCa9XknS9vlksSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGDRIESZ6U5ItJrk5ywhA1SJJGJh4ESXYG3go8GTgQOCbJgZOuQ5I0MsQRwSOAq6vqq1V1M/CfwJED1CFJAjYMsM77AN+aM34N8Mj5MyXZDGzuRm9K8sU5H28Erh9bhcNaM23L6xc1+5pp1yKtqXa5zYA11K4V2F779vniEEHQS1VtAbZs77MkM1U1PeGSJmK9ts12rT3rtW2269cNcWro28A+c8bv202TJA1giCD4X+ABSfZLsitwNHDWAHVIkhjg1FBV3ZLkz4GPATsD76iqKxe5mO2eMlon1mvbbNfas17bZrvmSVWtZCGSpDXGJ4slqXEGgSQ1blUHwY66okhyXJLZJJd2Py8Yos7FSvKOJNcluWKBz5Pkn7t2X5bk4EnXuBQ92vW4JDfO2V5/O+kalyLJPknOS7I1yZVJXrKdedbcNuvZrrW6ze6c5HNJPt+17e+2M8+dkpzebbMLk0wNUOqi9GzX4veLVbUqfxhdSP4KcD9gV+DzwIHz5jkOeMvQtS6hbY8FDgauWODzpwAfBQIcClw4dM0r1K7HAR8eus4ltGtv4OBu+C7Al7bzt7jmtlnPdq3VbRZgz254F+BC4NB587wIOLkbPho4fei6V6hdi94vruYjgnXbFUVVXQB8/w5mORJ4d418Frhbkr0nU93S9WjXmlRV11bVxd3wj4CrGD0hP9ea22Y927Umddvhpm50l+5n/p0xRwLv6obPAA5PkgmVuCQ927VoqzkIttcVxfb+SJ/RHYqfkWSf7Xy+FvVt+1p0WHdY+9Ekvz10MYvVnT54GKP/E5trTW+zO2gXrNFtlmTnJJcC1wHnVtWC26yqbgFuBO450SKXoEe7YJH7xdUcBH18CJiqqocA5/KrdNfqdDGwb1U9FPgX4L+GLWdxkuwJfAA4vqp+OHQ9K2UH7Vqz26yqbq2qgxj1XvCIJA8auKQV0aNdi94vruYg2GFXFFV1Q1X9vBs9BThkQrWN27rshqOqfrjtsLaqzgZ2SbJx4LJ6SbILo53lqVV15nZmWZPbbEftWsvbbJuq+gFwHvCkeR/dts2SbADuCtww0eKWYaF2LWW/uJqDYIddUcw7B3sEo3Oc68FZwHO7O1EOBW6sqmuHLmq5ktxr2znYJI9g9Pe36v/hdTW/Hbiqqt60wGxrbpv1adca3mabktytG94NeCLwhXmznQU8rxs+CvhkdVdbV6s+7VrKfnE19z663a4okvw9MFNVZwEvTnIEcAuji5THDVbwIiQ5jdHdGBuTXAO8mtFFH6rqZOBsRnehXA38BPijYSpdnB7tOgp4YZJbgJ8CR6/2f3idRwPHApd352YB/gb4TVjT26xPu9bqNtsbeFdGL8LaCXhfVX143v7j7cB7klzNaP9x9HDl9tanXYveL9rFhCQ1bjWfGpIkTYBBIEmNMwgkqXEGgSQ1ziCQpMYZBFqXkryy653xsq4Hxkcu8vvHJbn3nPFTkhy4gvX9QZJK8sA506ayQM+t0jgZBFp3khwGPI1Rz5oPAZ7A7fsB2tH3d2Z07/VtQVBVL6iqrStY5jHAp7vf0qAMAq1HewPXb3vMvqqur6rvACQ5PMklSS7P6P0Jd+qmfz3J65NczGjnPA2c2h1N7Jbk/CTT3bw3JfmHriO2zybZq5u+fzd+eZLXJrlpe8V1ffs8Bng+CzzElGT3JO/L6F0BH8yov/zpFf2vJHUMAq1HHwf2SfKlJCcl+V0YvdQDeCfwrKp6MKMn618453s3VNXBVfVeYAZ4dlUdVFU/nbf8PYDPdh2xXQD8STf9zcCbu2Vfcwf1HQmcU1VfAm5Isr2+YF4E/F9VHQi8ivXTj5ZWIYNA607XSdohwGZgFjg9yXHAAcDXuh0wjHplfOycr57ecxU3Ax/uhi8Cprrhw4D3d8P/cQffP4bR+zXofm/v9NBjts1TVVcAl/WsTVq0VdvXkLQcVXUrcD5wfpLLGXUudskOvvbjnov/xZz+dm5lEf+OktwDeDzw4CTFqB+tSvLyvsuQVppHBFp3khyQ5AFzJh0EfAP4IjCV5P7d9GOBTy2wmB8xen3jYnwWeEY3vFAHZkcB76mqfatqqqr2Ab4G/M68+T4DPBOgu1vpwYusRerNINB6tCejHhq3JrkMOBB4TVX9jFGvoO/vjhJ+CZy8wDLeCZy87WJxz/UeD7ysW+f9Gb3xar5jgA/Om/YBfv300EnApiRbgdcCVy6wPGnZ7H1UWiFJdgd+WlWV5GjgmKpa0nu2u1tYd6mqnyXZH/gEcED3/m5pRXmNQFo5hwBv6V7k8gPgj5exrN2B8zJ6g1iAFxkCGhePCCSpcV4jkKTGGQSS1DiDQJIaZxBIUuMMAklq3P8D1QywCZZZfXoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[3.918499735000296, 4.481905385000573, 9.614841139000418]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plotting the running times of each algorithm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ylabel('seconds')\n",
    "plt.xlabel('Sorting Alg')\n",
    "plt.bar([1,2,3],Times)\n",
    "plt.show()\n",
    "Times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf7b2ce",
   "metadata": {},
   "source": [
    "As we can see, the InsertionSort algorithm is the best one of the non MapReduced algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ff5b8179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing a MapReduced algorithm to confront the running times\n",
    "\n",
    "def MergeSort(Rank,h):\n",
    "    \n",
    "    if len(Rank) == 1:\n",
    "        return Rank\n",
    "\n",
    "    l = len(Rank) // 2\n",
    "    left = MergeSort(Rank[:l],h)\n",
    "    right = MergeSort(Rank[l:],h)\n",
    "    return merge(left, right,h)\n",
    "#####################################\n",
    "def merge(left, right,h):\n",
    "    merged = []\n",
    "    i = 0 \n",
    "    j=0\n",
    "    \n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i][h] > right[j][h]:\n",
    "            merged.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            merged.append(right[j])\n",
    "            j += 1\n",
    "    merged.extend(left[i:])\n",
    "    merged.extend(right[j:])\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d04d8f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rank=unsorted #because the Rank list was already sorted by previous codes so I'll assign back the unsorted one\n",
    "start=timeit.default_timer()\n",
    "#sorting Rank by scores\n",
    "Rank=MergeSort(Rank,1)\n",
    "#sorting duplicates by names (Reusing previous algorithm)\n",
    "i=0 \n",
    "while i <= (len(Rank)-1):\n",
    "        j=1\n",
    "        while (i+j)<(len(Rank)-1) and Rank[i][1]==Rank[i+j][1]: #to know the length of consecutive elements that have same score\n",
    "            j+=1\n",
    "        if j!=1: \n",
    "            #temp variable to store the correct order (we used [::-1] to sort in ascending order)\n",
    "            temp=MergeSort([Rank[i+k] for k in range(j)],0)[::-1] \n",
    "           \n",
    "            for k in range(j):\n",
    "                Rank[i+k]=temp[k] #changing the order in the Rank\n",
    "            i+=j #just to skip the elements already changed\n",
    "        else:\n",
    "            i+=1\n",
    "stop=timeit.default_timer()\n",
    "Times.append(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "2c6a80a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[3.918499735000296, 4.481905385000573, 9.614841139000418, 0.09068306100016343]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plotting the running times\n",
    "plt.ylabel('seconds')\n",
    "plt.xlabel('Sorting Alg')\n",
    "plt.bar([1,2,3,4],Times)\n",
    "plt.show()\n",
    "Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "ef20dccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Hunley 26.02\n",
      "Robert Seelbach 25.94\n",
      "Felicia Payne 25.88\n",
      "Karen Viard 25.88\n",
      "Nakia Loza 25.82\n",
      "Walter Jones 25.76\n",
      "Jasmine Harrell 25.72\n",
      "Mary Perrella 25.66\n",
      "Stanley Hartrick 25.64\n",
      "Samuel Winfrey 25.6\n"
     ]
    }
   ],
   "source": [
    "#TOP TEN STUDENTS\n",
    "assert len(Rank)>=10\n",
    "\n",
    "for i in range(10):\n",
    "    print(Rank[i][0]+' '+str(Rank[i][1]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "4ceee9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "F=open('RankingList.txt','w') #creating a .txt file with the sorted Applicants\n",
    "for i in range(len(Rank)):\n",
    "    F.write(Rank[i][0]+' '+str(Rank[i][1])+'\\n')\n",
    "F.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
